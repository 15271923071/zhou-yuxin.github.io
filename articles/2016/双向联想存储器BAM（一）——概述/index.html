<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>双向联想存储器BAM（一）——概述</title>
		<link rel="stylesheet" type="text/css" media="all" href="../../../style.css">
	</head>
	<body class="post-template-default single single-post postid-14 single-format-standard logged-in admin-bar single-author singular two-column left-sidebar customize-support">
		<div id="page" class="hfeed">
			<div id="main">
				<div id="primary">
					<div id="content" role="main">		
						<article id="post-14" class="post-14 post type-post status-publish format-standard hentry category-18">
							<header class="entry-header">
								<h1 class="entry-title">双向联想存储器BAM（一）——概述</h1>
							</header>
							<div class="entry-content">

<p>最近在研究《神经网络及其应用》一书，被里面的各种想法所吸引。第一个吸引我的就是双向联系存储器（Bidirectional Associative Memory，即BAM）。</p>

<p>=====================阶段一：基本原理=====================</p>

<p>人脑有一个特征就是，可以通过一个不完整或者有少量错误的信息中回想起一个完整的、正确的信息。比如听到一段旋律就能想到歌词，或是看到一个错别字但依旧能够理解句子意思。这种能力就称作联想能力。</p>

<p>联系存储器（Associative Memory，即AM）是指一类专门用来模拟联系记忆的神经网络。其结构如图所示：</p>

<p><img src="1.png"></p>

<p>假设该AM的输入维度为5，输出维度为4。当向AM输入一个5维行向量A(a[0],a[1],a[2],a[3],a[4])时，AM就会输出一个4维列向量B(b[0],b[1],b[2],b[3])，并有：</p>

<p>b[0]=a[0]*w[0][0]+a[1]*w[1][0]+a[2]*w[2][0]+a[3]*w[3][0]+a[4]*w[4][0]，</p>

<p>b[1]=a[0]*w[0][1]+a[1]*w[1][1]+a[2]*w[2][1]+a[3]*w[3][1]+a[4]*w[4][1]，</p>

<p>b[2]=a[0]*w[0][2]+a[1]*w[1][2]+a[2]*w[2][2]+a[3]*w[3][2]+a[4]*w[4][2]，</p>

<p>b[3]=a[0]*w[0][3]+a[1]*w[1][3]+a[2]*w[2][3]+a[3]*w[3][3]+a[4]*w[4][3]，</p>

<p>或者简写为：</p>

<p><img src="2.png"></p>

<p>其中w[i][j]就是图中灰色部分的连接的权值。</p>

<p>因此，一个AM网络可以表示成一个n行m列的矩阵W，输入一个n维向量A，输出一个m维向量B，其中有：</p>

<p>A*W=B。</p>

<p>====================阶段二：学习法则=================</p>

<p>AM中的权值矩阵是如何确定的呢？一开始，权值矩阵W=0，即所有权重都为0。</p>

<p>假设要记忆L对向量(A1,B1)、(A2,B2)、(A3,B3)&#8230;(AL,BL)，那么W就变为：</p>

<p><img src="3.png"></p>

<p>如果要通过Ai从AM中回忆出Bi，那么就执行乘法运算：</p>

<p><img src="4.png"></p>

<p>如果L个Ai均是正交的，即</p>

<p><img src="5.png"></p>

<p>那么乘法运算的结果就是Bi。</p>

<p>通常我们的输入不会保证正交。如果输入不正交，那么输出就会产生误差，或者称作“串音”。输入Ai之间的相关度越大，串音就越严重。</p>

<p>===================阶段三：消除串音与噪音====================</p>

<p>在阶段二中已经得知，由于输入之间不正交，输出就会产生串音。事实上，实际应用中，输入的总是被噪音污染了的或发生了相当程度的畸变的Ai。于是，如何消除串音与噪音成了AM的重要课题。</p>

<p>该课题的突破口就是将联想存储器AM进化成双向联想存储区BAM。BAM操作的基本流程为：</p>

<p>（1）向BAM输入被噪声污染或者发生了相当程度的畸变的Ai‘；</p>

<p>（2）通过乘法Ai’W产生了一个有一定噪声与串音的Bi&#8217;；</p>

<p>（3）把Bi&#8217;重新代入W中，通过乘法Bi&#8217;W<sup>T</sup>，产生Ai&#8221;；</p>

<p>（4）把Ai&#8221;重新带入W中，通过乘法Ai&#8221;W，产生Bi&#8221;；</p>

<p>（5）把Bi&#8221;重新带入W中，通过乘法Bi&#8221;W<sup>T</sup>，产生Ai‘’‘；</p>

<p>……如此不断迭代，直到输出不再变化为止，取此时的输出为Af与Bf。</p>

<p>通常，Af与Bf即为本来存入的完整、正确的信息，或者是足够接近原本存入的信息。</p>

<p>BAM奏效的基本假设是，噪音与串音所占的成分不是太大，于是，每次迭代，杂音的部分就逐渐收敛，信噪比越来越高，最终正向与反向信息流稳定下来，或按一个固定的数据对谐振。具体的理论证明可以查看Grossberg的自适应谐振理论《A massively parallel Architecture for a Self-organizing Neural Pattern Recognition Machine》。</p>

<p>======================阶段四：实际使用======================</p>

<p>BAM用的最多的就是对二进制向量的记忆与取出。如果直接输入二进制向量Ai与Bi，那么权值矩阵按照AM的学习规则：</p>

<p><img src="6.png"></p>

<p>于是，W中不可能存在负值的权重。这违反了Hebb学习规则中既有兴奋性连接又要有抑制性连接的要求。</p>

<p>解决此矛盾的办法是将二进制向量变换成双极向量，即用-1替代原理的0，比如向量二进制向量(0 0 1 0 1 1 0 1)要变成(-1 -1 1 -1 1 1 -1 1)，这样就与Hebb学习规则一致了。</p>

<p>下面举例说明BAM的编码和译码过程。</p>

<p>假设一个BAM的输入维度是6，输出维度是4，现有两个联想对(A1,B1)、(A2,B2)：</p>

<p>A1=(1 0 1 0 1 0)，B1=(1 1 0 0)</p>

<p>A2=(1 1 1 0 0 0)，B2=(1 0 1 0)</p>

<p>先将它们转换成双极对：</p>

<p>X1=(1 -1 1 -1 1 -1)，Y1=(1 1 -1 -1)</p>

<p>X2=(1 1 1 -1 -1 -1)，Y2=(1 -1 1 -1)</p>

<p>再将这两个双极向量对转换成两个双极关联矩阵：</p>

<p><img src="7.png"></p>

<p><img src="8.png"></p>

<p>然后把这两个矩阵相加得到BAM的权值矩阵W：</p>

<p><img src="9.png"></p>

<p>至此，BAM学习结束。</p>

<p>此时如果输入A1=(1 0 1 0 1 0)，执行乘法A1W=(4 2 -2 -4)，应用阈值规则，结果为(1 1 0 0)，也就是B1。</p>

<p>如果输入带有杂音的A=(0 1 1 0 0 0)，即A2被扰乱了1位后的结果，则运算结果为(2 -2 2 -2)，应用阈值规则，结果为(1 0 1 0)，即B2。这说明BAM有一定的容错能力。</p>

<p>当然，这里是举的简单的例子，当BAM包含的神经元数据较多并且记忆的联系对也较多时，一个噪声模式输入后，要经过多次迭代，才能达到稳定，正确回忆出某个联想对来，或得到的输出最接近与希望回忆出来的联想对。</p>

			﻿							</div>
						</article>
					</div>
				</div>
			</div>
			<footer id="colophon" role="contentinfo">
				<div id="site-generator">周坚石@南京大学软件学院 504849766@qq.com</div>
			</footer>
		</div>
	</body>
</html>